{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1_WKdcrI6w3"
   },
   "source": [
    "# PPO and ProcGen notebbok\n",
    "\n",
    "In this notebook a runnable code (with some exceptions) is accessible, first the import of neccessary packages and code snippest from the repository are imported. Setting up the parameters for the training loop and running the training loop with different setups is performed. The information gained from training (updated policy) are then used to play on unseen data and the results plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7LP1JU3I-d4"
   },
   "source": [
    "The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. Python scripts including functions for data augmentations are imported for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdpZ4lmFHtD8",
    "outputId": "e6824c2f-0d4d-4e9e-eb30-c78b31feec49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: procgen in /usr/local/lib/python3.6/dist-packages (0.10.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (1.19.4)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (3.0.12)\n",
      "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.17.3)\n",
      "Requirement already satisfied: gym3<1.0.0,>=0.3.3 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.3.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.14.4)\n",
      "Requirement already satisfied: glfw<2.0.0,>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.12.0)\n",
      "Requirement already satisfied: moderngl<6.0.0,>=5.5.4 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (5.6.2)\n",
      "Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.9.0)\n",
      "Requirement already satisfied: imageio-ffmpeg<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (0.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.20)\n",
      "Requirement already satisfied: glcontext<3,>=2 in /usr/local/lib/python3.6/dist-packages (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen) (2.2.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (7.0.0)\n",
      "--2020-12-27 18:08:21--  https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14807 (14K) [text/plain]\n",
      "Saving to: ‘utils.py.1’\n",
      "\n",
      "utils.py.1          100%[===================>]  14.46K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-12-27 18:08:22 (53.2 MB/s) - ‘utils.py.1’ saved [14807/14807]\n",
      "\n",
      "--2020-12-27 18:08:22--  https://raw.githubusercontent.com/hlynurarni/Deep_learning_02456_FProject/master/HPC%20Scripts/data_aug.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4697 (4.6K) [text/plain]\n",
      "Saving to: ‘data_aug.py.1’\n",
      "\n",
      "data_aug.py.1       100%[===================>]   4.59K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-12-27 18:08:22 (64.0 MB/s) - ‘data_aug.py.1’ saved [4697/4697]\n",
      "\n",
      "--2020-12-27 18:08:22--  https://raw.githubusercontent.com/hlynurarni/Deep_learning_02456_FProject/master/HPC%20Scripts/TransformLayer.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7799 (7.6K) [text/plain]\n",
      "Saving to: ‘TransformLayer.py.1’\n",
      "\n",
      "TransformLayer.py.1 100%[===================>]   7.62K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-12-27 18:08:22 (101 MB/s) - ‘TransformLayer.py.1’ saved [7799/7799]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install procgen\n",
    "!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
    "!wget https://raw.githubusercontent.com/hlynurarni/Deep_learning_02456_FProject/master/HPC%20Scripts/data_aug.py\n",
    "!wget https://raw.githubusercontent.com/hlynurarni/Deep_learning_02456_FProject/master/HPC%20Scripts/TransformLayer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn2rkllGJPtZ"
   },
   "source": [
    "Hyperparameters, the value can be changed. For ease of read and to showcase a working notebook 20.000 steps are made for few different combinations. Initial values, which are a good starting point can be seen in comments for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8Z8P1ehENCwc"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "total_steps = 1e2 # 8e6\n",
    "num_envs = 32 # 32\n",
    "num_levels = 100 # 10\n",
    "num_steps = 256 # 256\n",
    "num_epochs = 3 # 3\n",
    "batch_size = 512 # 512\n",
    "eps = .2 # .2\n",
    "grad_eps = .5 # .5\n",
    "value_coef = .5 # .5\n",
    "entropy_coef = .01 # .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store the data the gdrive is mounted, this can be altered and changed based on the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3Ast46jAThS",
    "outputId": "dc1f04e3-de00-4fa8-8761-2a5a51e13d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxRWy_T9JY4M"
   },
   "source": [
    "The policy network using popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. Here the neccessary libraries are imported, along with the data agumentations functions from the imported python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hiFZgIv3e7Yp"
   },
   "outputs": [],
   "source": [
    "# Add the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import make_env, Storage, orthogonal_init\n",
    "from data_aug import RandGray,random_color_jitter,random_cutout\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTtJoX1JPGsJ"
   },
   "source": [
    "Implementation of the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) minus the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r9Y6gJomjKbf"
   },
   "outputs": [],
   "source": [
    "# Impala encoder\n",
    "#  Added instead of the given Encoder \n",
    "def xavier_uniform_init(module, gain=1.0):\n",
    "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(module.weight.data, gain)\n",
    "        nn.init.constant_(module.bias.data, 0)\n",
    "    return module\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(x)\n",
    "        out = self.conv1(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "\n",
    "class ImpalaBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ImpalaBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.res1 = ResidualBlock(out_channels)\n",
    "        self.res2 = ResidualBlock(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,in_channels,out_features,**kwargs):\n",
    "        super().__init__()\n",
    "        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n",
    "        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n",
    "        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n",
    "        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=out_features)\n",
    "\n",
    "        self.output_dim = feature_dim\n",
    "        self.apply(xavier_uniform_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = Flatten()(x)\n",
    "        x = self.fc(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ptG8simc6l3U"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    \"\"\" show an image \"\"\"\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ba6TZ6UgXl1f"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class Encoder2(nn.Module):\n",
    "    def __init__(self, in_channels, feature_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
    "        )\n",
    "        self.apply(orthogonal_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, encoder, feature_dim, num_actions):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
    "    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
    "\n",
    "    def act(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x.cuda().contiguous()\n",
    "            dist, value = self.forward(x)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.cpu(), log_prob.cpu(), value.cpu()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        logits = self.policy(x)\n",
    "        value = self.value(x).squeeze(1)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "adv_b-6VYXVQ"
   },
   "outputs": [],
   "source": [
    "#  Example runs\n",
    "setup_run = {\n",
    "    'Run1': {\n",
    "      'Encoder':'regular',\n",
    "      'Data_aug': 'regular',\n",
    "      'Mixreg': False,\n",
    "      'Policy': None\n",
    "    }\n",
    "}\n",
    "\n",
    "setup_run['Run2'] = {\n",
    "    'Encoder':'impala',\n",
    "    'Data_aug': 'regular',\n",
    "    'Mixreg': False,\n",
    "    'Policy': None\n",
    "}\n",
    "\n",
    "setup_run['Run3'] = {\n",
    "    'Encoder':'impala',\n",
    "    'Data_aug': 'grayscale',\n",
    "    'Mixreg': False,\n",
    "    'Policy': None\n",
    "}\n",
    "\n",
    "setup_run['Run4'] = {\n",
    "    'Encoder':'impala',\n",
    "    'Data_aug': 'color_jitter',\n",
    "    'Mixreg': False,\n",
    "    'Policy': None\n",
    "}\n",
    "\n",
    "setup_run['Run5'] = {\n",
    "    'Encoder':'impala',\n",
    "    'Data_aug': 'cut_out',\n",
    "    'Mixreg': False,\n",
    "    'Policy': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTBV9xpKpEFa",
    "outputId": "b3fce6f7-5e4a-4f19-b757-126feb2fa6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
      "Action space: 15\n",
      "Run setup for Run1: playing starpilot\n",
      "Encoder: regular\tData_aug: regular\tMixreg: False\tPolicy: None\t\n",
      "Step: 8192\tMean reward: 8.401390075683594, \tMean reward done: 2.391955614089966\n",
      "Estimated time of completion: 3.1214913017013006 hours\n",
      "Completed training of Run1!\n",
      "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
      "Action space: 15\n",
      "Run setup for Run2: playing starpilot\n",
      "Encoder: impala\tData_aug: regular\tMixreg: False\tPolicy: None\t\n",
      "Using Impala\n",
      "Step: 8192\tMean reward: 8.384974479675293, \tMean reward done: 2.3830788135528564\n",
      "Estimated time of completion: 4.3119494027147685 hours\n",
      "Completed training of Run2!\n",
      "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
      "Action space: 15\n",
      "Run setup for Run3: playing starpilot\n",
      "Encoder: impala\tData_aug: grayscale\tMixreg: False\tPolicy: None\t\n",
      "Using Impala\n",
      "Step: 8192\tMean reward: 8.401407241821289, \tMean reward done: 2.3919591903686523\n",
      "Estimated time of completion: 4.396456578332517 hours\n",
      "Completed training of Run3!\n",
      "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
      "Action space: 15\n",
      "Run setup for Run4: playing starpilot\n",
      "Encoder: impala\tData_aug: color_jitter\tMixreg: False\tPolicy: None\t\n",
      "Using Impala\n",
      "Step: 8192\tMean reward: 8.384839057922363, \tMean reward done: 2.3829822540283203\n",
      "Estimated time of completion: 5.815249880672329 hours\n",
      "Completed training of Run4!\n",
      "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
      "Action space: 15\n",
      "Run setup for Run5: playing starpilot\n",
      "Encoder: impala\tData_aug: cut_out\tMixreg: False\tPolicy: None\t\n",
      "Using Impala\n",
      "Step: 8192\tMean reward: 8.384974479675293, \tMean reward done: 2.3830788135528564\n",
      "Estimated time of completion: 4.255900340568688 hours\n",
      "Completed training of Run5!\n",
      "Completed all runs!\n"
     ]
    }
   ],
   "source": [
    "#  ============================       RUNNING LOOP      ==============================\n",
    "\n",
    "run_setups =['Run1','Run2','Run3','Run4','Run5'] # Specify a list of setups to run\n",
    "\n",
    "for run in run_setups: # Run the training for all our setups\n",
    "    # Define environment\n",
    "    # check the utils.py file for info on arguments\n",
    "    env_name = 'starpilot'\n",
    "    env = make_env(num_envs, num_levels=num_levels, env_name='starpilot')\n",
    "    print('Observation space:', env.observation_space)\n",
    "    print('Action space:', env.action_space.n)\n",
    "    print(f'Run setup for {run}: playing {env_name}')\n",
    "    for setup_key in setup_run[run]:\n",
    "        print(f'{setup_key}: {setup_run[run][setup_key]}', end = '\\t')\n",
    "    print('')\n",
    "\n",
    "    # Read in the setup\n",
    "    do_mixreg = setup_run[run]['Mixreg']\n",
    "    data_aug = setup_run[run]['Data_aug']\n",
    "    encoder_use = setup_run[run]['Encoder']\n",
    "\n",
    "    # Define network\n",
    "    feature_dim = 512\n",
    "    lambda_mix = 0.95\n",
    "    num_actions = env.action_space.n\n",
    "    in_channels = env.observation_space.shape[0]\n",
    "\n",
    "    # Define the encoder\n",
    "    if encoder_use == 'impala':\n",
    "        print('Using Impala') \n",
    "        encoder = Encoder(in_channels, feature_dim) # added\n",
    "    else:\n",
    "        encoder = Encoder2(in_channels, feature_dim) # added\n",
    "\n",
    "    # Initialize the policy\n",
    "    policy = Policy(encoder, feature_dim, num_actions) # added\n",
    "    policy.cuda()\n",
    "\n",
    "    # Define optimizer\n",
    "    # these are reasonable values but probably not optimal\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
    "\n",
    "    # Define temporary storage\n",
    "    # we use this to collect transitions during each iteration\n",
    "    storage = Storage(\n",
    "          env.observation_space.shape,\n",
    "          num_steps,\n",
    "          num_envs\n",
    "      )\n",
    "\n",
    "    # Run training\n",
    "    obs = env.reset()\n",
    "    nenv = env.num_envs\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    # Change the first observations to desired augmentation\n",
    "    if data_aug == 'grayscale':\n",
    "        obs = np.zeros((nenv,) + env.observation_space.shape, dtype=env.observation_space.dtype.name)\n",
    "        obs[:] = env.reset()\n",
    "\n",
    "        # Do the grayscale and transfer to tensor\n",
    "        augs_funcs = RandGray(batch_size=num_envs, p_rand=1) # added\n",
    "        obs = augs_funcs.do_augmentation(obs)\n",
    "        obs = torch.from_numpy(obs)\n",
    "    elif data_aug == 'random_cutout':\n",
    "        # Initialize as a numpy array then convert to tensor\n",
    "        obs = np.zeros((nenv,) + env.observation_space.shape, dtype=env.observation_space.dtype.name)\n",
    "        obs[:] = env.reset()\n",
    "\n",
    "        # Do the cutout and transfer to tensor\n",
    "        obs = random_cutout(obs,12,24)\n",
    "        obs = torch.from_numpy(obs)\n",
    "    elif data_aug == 'color_jitter':\n",
    "        obs = random_color_jitter(obs,p=0.5)\n",
    "\n",
    "    step = 0\n",
    "    # Initilize mean_reward for each setup that we store in the end\n",
    "    mean_rewards = []\n",
    "    mean_rewards_done = []\n",
    "    first_loop = True\n",
    "    start = time() # Lets measure how long each training task takes\n",
    "    while step < total_steps:\n",
    "        # Use policy to collect data for num_steps steps\n",
    "        policy.eval()\n",
    "        for _ in range(num_steps):\n",
    "            # Use policy\n",
    "            action, log_prob, value = policy.act(obs)\n",
    "\n",
    "            # Take step in environment\n",
    "            next_obs = np.zeros((nenv,) + env.observation_space.shape, dtype=env.observation_space.dtype.name)\n",
    "\n",
    "            # numpy obs\n",
    "            next_obs[:], reward, done, info = env.step(action)\n",
    "\n",
    "            # Store data\n",
    "            storage.store(obs, action, reward, done, info, log_prob, value)\n",
    "\n",
    "            # Make augmented transformation, probably possible to do this another way, like in a class to avoid the if statements\n",
    "            if data_aug == 'grayscale':\n",
    "                obs = augs_funcs.do_augmentation(next_obs)\n",
    "                obs = torch.from_numpy(obs)\n",
    "            elif data_aug == 'random_cutout':\n",
    "                obs = random_cutout(next_obs,12,24)\n",
    "                obs = torch.from_numpy(obs)\n",
    "            elif data_aug == 'color_jitter':\n",
    "                obs = torch.from_numpy(next_obs)\n",
    "                obs = random_color_jitter(obs,p=0.5)\n",
    "            else:\n",
    "                obs = torch.from_numpy(next_obs)\n",
    "\n",
    "\n",
    "        # Add the last observation to collected data\n",
    "        _, _, value = policy.act(obs)\n",
    "        storage.store_last(obs, value)\n",
    "\n",
    "        # Compute return and advantage\n",
    "        storage.compute_return_advantage()\n",
    "\n",
    "        # Optimize policy\n",
    "        policy.train()\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            # Iterate over batches of transitions\n",
    "            generator = storage.get_generator(batch_size)\n",
    "            for batch in generator:\n",
    "                b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
    "\n",
    "                if do_mixreg: \n",
    "                    index_ij = torch.randint(0, batch_size-1, (batch_size,2))\n",
    "                    b_obs = lambda_mix*b_obs[index_ij[:,0]] + (1-lambda_mix)*b_obs[index_ij[:,1]]\n",
    "                    b_log_prob = lambda_mix*b_log_prob[index_ij[:,0]] + (1-lambda_mix)*b_log_prob[index_ij[:,1]]\n",
    "                    b_value = lambda_mix*b_value[index_ij[:,0]] + (1-lambda_mix)*b_value[index_ij[:,1]]\n",
    "                    b_returns = lambda_mix*b_returns[index_ij[:,0]] + (1-lambda_mix)*b_returns[index_ij[:,1]]\n",
    "                    b_advantage = lambda_mix*b_advantage[index_ij[:,0]] + (1-lambda_mix)*b_advantage[index_ij[:,1]]\n",
    "                    if (lambda_mix >= 0.5):\n",
    "                        b_action = b_action[index_ij[:,0]]\n",
    "                    else:\n",
    "                        b_action = b_action[index_ij[:,1]]\n",
    "\n",
    "            # Get current policy outputs\n",
    "            new_dist, new_value = policy(b_obs)\n",
    "            new_log_prob = new_dist.log_prob(b_action)\n",
    "\n",
    "            # Clipped policy objective\n",
    "            ratio = torch.exp(new_log_prob - b_log_prob) # added\n",
    "            # ratio = b_log_prob/new_log_prob # added\n",
    "            clipped_ratio = ratio.clamp(min=1.0 - eps,max=1.0 + eps) # added\n",
    "            # pi_loss = torch.min(rt_theta*b_advantage,) # added\n",
    "            pi_loss = -torch.min(ratio * b_advantage,clipped_ratio * b_advantage).mean() # added\n",
    "\n",
    "            # Clipped value function objective\n",
    "            clipped_value = b_value + (new_value - b_value).clamp(min=-eps, max=eps) # added\n",
    "            # value_loss = (new_value - b_value)**2 # added\n",
    "            value_loss = 0.5 * torch.max((b_value - b_returns) ** 2, (clipped_value - b_returns) **2).mean() # added\n",
    "\n",
    "            # Entropy loss\n",
    "            entropy_loss = -new_dist.entropy().mean() # added\n",
    "\n",
    "            # Backpropagate losses\n",
    "            loss = pi_loss + value_coef*value_loss + entropy_coef*entropy_loss # added\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps) # added\n",
    "\n",
    "            # Update policy\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Update stats\n",
    "        mean_rewards.append(storage.get_reward(normalized_reward=False))\n",
    "        done_reward = sum((sum(storage.reward)/(sum(storage.done)+1)))/num_envs\n",
    "\n",
    "        # TODO: If you never die implement an if statement that doesn't include the plus 1\n",
    "        mean_rewards_done.append(done_reward)\n",
    "        step += num_envs * num_steps\n",
    "        print(f'Step: {step}\\tMean reward: {storage.get_reward(normalized_reward=False)}, \\tMean reward done: {done_reward}')\n",
    "        if first_loop:\n",
    "            end = time()\n",
    "            time_total = end-start\n",
    "            estimated_time = (8e6/(8192/time_total))/3600\n",
    "            print(f'Estimated time of completion: {estimated_time} hours')\n",
    "        first_loop = False\n",
    "        \n",
    "    # While loop ended, save results\n",
    "    end = time()\n",
    "    time_total = end-start\n",
    "    \n",
    "    # Save the newest version after every epoch\n",
    "    torch.save({\n",
    "              'Setup': setup_run[run], # Have \n",
    "              'policy_state_dict': policy.state_dict(), # This is the policy\n",
    "              'encoder_state_dict': encoder.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(), # The optimizer used\n",
    "              'Mean Reward': mean_rewards,\n",
    "              'Mean Reward Done': mean_rewards_done,\n",
    "              'Training time': time_total,\n",
    "              }, f'/content/gdrive/MyDrive/Deep Learning Project 2020/data/{run}.pt')\n",
    "    print(f'Completed training of {run}!')\n",
    "    torch.save(policy.state_dict(), f'{run}_policy.pt')\n",
    "\n",
    "print('Completed all runs!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PPO-training-notebooketting.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

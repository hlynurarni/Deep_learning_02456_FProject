{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_Additional_FCL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_WKdcrI6w3"
      },
      "source": [
        "# Getting started with PPO and ProcGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LP1JU3I-d4"
      },
      "source": [
        "Here's a bit of code that should help you get started on your projects.\n",
        "\n",
        "The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdpZ4lmFHtD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc03e6ba-f580-420f-c50e-6318902fab31"
      },
      "source": [
        "!pip install procgen\n",
        "!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
        "# !wget https://raw.githubusercontent.com/pokaxpoka/rad_procgen/346fb852fa16e739601ca998eebd1d56b95aa2e8/train_procgen/data_augs.py\n",
        "!wget https://raw.githubusercontent.com/MishaLaskin/rad/master/TransformLayer.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: procgen in /usr/local/lib/python3.6/dist-packages (0.10.4)\n",
            "Requirement already satisfied: gym3<1.0.0,>=0.3.3 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.3.3)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.17.3)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (3.0.12)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (1.18.5)\n",
            "Requirement already satisfied: moderngl<6.0.0,>=5.5.4 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (5.6.2)\n",
            "Requirement already satisfied: imageio-ffmpeg<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (0.3.0)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.9.0)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.14.3)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n",
            "Requirement already satisfied: glcontext<3,>=2 in /usr/local/lib/python3.6/dist-packages (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen) (2.2.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (7.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n",
            "--2020-12-03 14:23:05--  https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14807 (14K) [text/plain]\n",
            "Saving to: ‘utils.py.6’\n",
            "\n",
            "utils.py.6          100%[===================>]  14.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-03 14:23:05 (127 MB/s) - ‘utils.py.6’ saved [14807/14807]\n",
            "\n",
            "--2020-12-03 14:23:05--  https://raw.githubusercontent.com/MishaLaskin/rad/master/TransformLayer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7657 (7.5K) [text/plain]\n",
            "Saving to: ‘TransformLayer.py.6’\n",
            "\n",
            "TransformLayer.py.6 100%[===================>]   7.48K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-03 14:23:05 (102 MB/s) - ‘TransformLayer.py.6’ saved [7657/7657]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn2rkllGJPtZ"
      },
      "source": [
        "Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z8P1ehENCwc"
      },
      "source": [
        "# Hyperparameters\n",
        "total_steps = 2e6 #1e4 #8e6  \n",
        "num_envs = 32 # 32\n",
        "num_levels = 100 # 10\n",
        "num_steps = 256 # 256\n",
        "num_epochs = 3 # 3\n",
        "batch_size = 512 # 512\n",
        "eps = .2 # .2\n",
        "grad_eps = .5 # .5\n",
        "value_coef = .5 # .5\n",
        "entropy_coef = .01 # .01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Ast46jAThS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4ee018-7606-4774-dd84-bf30796db645"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxRWy_T9JY4M"
      },
      "source": [
        "Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiFZgIv3e7Yp"
      },
      "source": [
        "# Add the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import make_env, Storage, orthogonal_init\n",
        "from time import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Y6gJomjKbf"
      },
      "source": [
        "# For Impala encoder\n",
        "def xavier_uniform_init(module, gain=1.0):\n",
        "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "        nn.init.xavier_uniform_(module.weight.data, gain)\n",
        "        nn.init.constant_(module.bias.data, 0)\n",
        "    return module\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(x)\n",
        "        out = self.conv1(out)\n",
        "        out = nn.ReLU()(out)\n",
        "        out = self.conv2(out)\n",
        "        return out + x\n",
        "\n",
        "class ImpalaBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ImpalaBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.res1 = ResidualBlock(out_channels)\n",
        "        self.res2 = ResidualBlock(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,in_channels,out_features,**kwargs):\n",
        "        super().__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n",
        "        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n",
        "        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n",
        "        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=out_features)\n",
        "\n",
        "        self.output_dim = feature_dim\n",
        "        self.apply(xavier_uniform_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = Flatten()(x)\n",
        "        x = self.fc(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgV9bnQzjMkl"
      },
      "source": [
        "# Augmentation code\n",
        "class RandGray(object):\n",
        "    def __init__(self,  \n",
        "                 batch_size, \n",
        "                 p_rand=0.5,\n",
        "                 *_args, \n",
        "                 **_kwargs):\n",
        "        \n",
        "        self.p_gray = p_rand\n",
        "        self.batch_size = batch_size\n",
        "        self.random_inds = np.random.choice([True, False], \n",
        "                                            batch_size, \n",
        "                                            p=[self.p_gray, 1 - self.p_gray])\n",
        "        \n",
        "    def grayscale(self, imgs):\n",
        "        # imgs: b x h x w x c\n",
        "        # the format is incorrect\n",
        "        b, c, h, w = imgs.shape # format changed, hlynur\n",
        "        imgs = imgs[:, 0, :, :] * 0.2989 + imgs[:, 1, :, :] * 0.587 + imgs[:, 2, :, :] * 0.114 \n",
        "        imgs = np.tile(imgs.reshape(b,-1,h,w), (1, 3, 1, 1)) # .astype(np.uint8)\n",
        "        return imgs\n",
        "\n",
        "    def do_augmentation(self, images):\n",
        "        # images: [B, C, H, W]\n",
        "        bs, channels, h, w = images.shape\n",
        "        # print(images.shape)\n",
        "        if self.random_inds.sum() > 0:\n",
        "            print(self.random_inds)\n",
        "            # print(sum(self.random_inds))\n",
        "            # print(images[self.random_inds].shape)\n",
        "            images[self.random_inds] =  self.grayscale(images[self.random_inds])\n",
        "\n",
        "        return images\n",
        "    \n",
        "    def change_randomization_params(self, index_):\n",
        "        self.random_inds[index_] = np.random.choice([True, False], 1, \n",
        "                                                    p=[self.p_gray, 1 - self.p_gray])\n",
        "        \n",
        "    def change_randomization_params_all(self):\n",
        "        self.random_inds = np.random.choice([True, False], \n",
        "                                            self.batch_size, \n",
        "                                            p=[self.p_gray, 1 - self.p_gray])\n",
        "        \n",
        "    def print_parms(self):\n",
        "        print(self.random_inds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqgUABtHEVUz"
      },
      "source": [
        "class Center_Crop(object):\n",
        "    def __init__(self, \n",
        "                 *_args, \n",
        "                 **_kwargs):\n",
        "        self.crop_size = 64\n",
        "    \n",
        "    def do_augmentation(self, image):\n",
        "        h, w = image.shape[1], image.shape[2]\n",
        "        new_h, new_w = self.crop_size, self.crop_size\n",
        "\n",
        "        top = (h - new_h)//2\n",
        "        left = (w - new_w)//2\n",
        "        image = image[:, top:top + new_h, left:left + new_w, :]\n",
        "        # print('returning image as:', image[0,:,:,:])\n",
        "        return image.copy()\n",
        "    \n",
        "    def change_randomization_params(self, index_):\n",
        "        index_ = index_\n",
        "        \n",
        "    def change_randomization_params_all(self):\n",
        "        index_ = 0\n",
        "    \n",
        "    def print_parms(self):\n",
        "        print('nothing')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdq0EHK2ctHd"
      },
      "source": [
        "from TransformLayer import ColorJitterLayer\n",
        "transform_module = nn.Sequential(ColorJitterLayer(brightness=0.4, \n",
        "                                                  contrast=0.4,\n",
        "                                                  saturation=0.4, \n",
        "                                                  hue=0.5, \n",
        "                                                  p=1.0, \n",
        "                                                  batch_size=num_envs,\n",
        "                                                  stack_size=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t653Cj_-MNZv"
      },
      "source": [
        "def color_jitter(obs):\n",
        "  \n",
        "  # device = torch.device('cpu')\n",
        "  in_stacked_x = obs.to(device)\n",
        "  # in_stacked_x= in_stacked_x / 255.0\n",
        "  # in_stacked_x = in_stacked_x.reshape(-1,3,64,64)\n",
        "  # start = time()\n",
        "  randconv_x = transform_module(obs)\n",
        "  # return (randconv_x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pyzvoalcewg"
      },
      "source": [
        "def grayscale(imgs,device):\n",
        "    # imgs: b x c x h x w\n",
        "    b, c, h, w = imgs.shape\n",
        "    # frames = c // 3\n",
        "    \n",
        "    # imgs = imgs.view([b,frames,3,h,w])\n",
        "    # imgs = imgs[:, 0, :, :] * 0.2989 + imgs[:, 1, :, :] * 0.587 + imgs[:, 2, :, :] * 0.114 \n",
        "    \n",
        "    imgs = imgs.view([b,3,h,w])\n",
        "    imgs = imgs[:, 0, ...] * 0.2989 + imgs[:, 1, ...] * 0.587 + imgs[:, 2, ...] * 0.114 \n",
        "    \n",
        "\n",
        "    # imgs = imgs.type(torch.uint8).float()\n",
        "    # assert len(imgs.shape) == 3, imgs.shape\n",
        "    imgs = imgs[:, None, :, :]\n",
        "    imgs = imgs * torch.ones([1, 3, 1, 1], dtype=imgs.dtype).float().to(device) # broadcast tiling\n",
        "    return imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQjn9hIP1kON"
      },
      "source": [
        "def random_cutout(imgs, min_cut,max_cut):\n",
        "    \"\"\"\n",
        "        args:\n",
        "        imgs: shape (B,C,H,W)\n",
        "        out: output size (e.g. 84)\n",
        "    \"\"\"\n",
        "    n, c, h, w = imgs.shape\n",
        "    w1 = np.random.randint(min_cut, max_cut, n)\n",
        "    h1 = np.random.randint(min_cut, max_cut, n)\n",
        "    \n",
        "    cutouts = np.empty((n, c, h, w), dtype=imgs.dtype)\n",
        "    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n",
        "        cut_img = img.copy()\n",
        "        cut_img[:, h11:h11 + h11, w11:w11 + w11] = 0\n",
        "        #print(img[:, h11:h11 + h11, w11:w11 + w11].shape)\n",
        "        cutouts[i] = cut_img\n",
        "    return cutouts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba6TZ6UgXl1f"
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class Encoder2(nn.Module):\n",
        "  def __init__(self, in_channels, feature_dim):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "        Flatten(),\n",
        "        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
        "    )\n",
        "    self.apply(orthogonal_init)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "  def __init__(self, encoder, feature_dim, num_actions):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "  def act(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = x.cuda().contiguous()\n",
        "      dist, value = self.forward(x)\n",
        "      action = dist.sample()\n",
        "      log_prob = dist.log_prob(action)\n",
        "    \n",
        "    return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    logits = self.policy(x)\n",
        "    value = self.value(x).squeeze(1)\n",
        "    dist = torch.distributions.Categorical(logits=logits)\n",
        "    \n",
        "    return dist, value\n",
        "\n",
        "class Policy_add_FCL(nn.Module):\n",
        "  def __init__(self, encoder, feature_dim, num_actions):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.policy_add_FCL = nn.Linear(feature_dim, int(feature_dim/2))\n",
        "    self.policy = orthogonal_init(nn.Linear(int(feature_dim/2), num_actions), gain=.01)\n",
        "    self.value_add_FCL = nn.Linear(feature_dim, int(feature_dim/2))\n",
        "    self.value = orthogonal_init(nn.Linear(int(feature_dim/2), 1), gain=1.)\n",
        "\n",
        "  def act(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = x.cuda().contiguous()\n",
        "      dist, value = self.forward(x)\n",
        "      action = dist.sample()\n",
        "      log_prob = dist.log_prob(action)\n",
        "    \n",
        "    return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    pol_add = F.relu(self.policy_add_FCL(x))\n",
        "    logits = self.policy(pol_add)\n",
        "    val_add = F.relu(self.value_add_FCL(x))\n",
        "    value = self.value(val_add).squeeze(1)\n",
        "    dist = torch.distributions.Categorical(logits=logits)\n",
        "    \n",
        "    return dist, value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adv_b-6VYXVQ"
      },
      "source": [
        "setup_run = {\n",
        "    'ex1': {\n",
        "      'Encoder':'impala',\n",
        "      'Data_aug': 'regular',\n",
        "      'Mixreg': False,\n",
        "      'Policy': None,\n",
        "      'add_FCL': False\n",
        "    }\n",
        "}\n",
        "\n",
        "setup_run['ex2'] = {\n",
        "    'Encoder':'impala',\n",
        "    'Data_aug': 'regular',\n",
        "    'Mixreg': False,\n",
        "    'Policy': None,\n",
        "    'add_FCL': False\n",
        "}\n",
        "\n",
        "setup_run['ex3'] = {\n",
        "    'Encoder':'impala',\n",
        "    'Data_aug': 'grayscale',\n",
        "    'Mixreg': False,\n",
        "    'Policy': None,\n",
        "    'add_FCL': False\n",
        "}\n",
        "\n",
        "setup_run['ex4'] = {\n",
        "    'Encoder':'impala',\n",
        "    'Data_aug': 'color_jitter',\n",
        "    'Mixreg': False,\n",
        "    'Policy': None,\n",
        "    'add_FCL': False\n",
        "}\n",
        "\n",
        "setup_run['ex5'] = {\n",
        "    'Encoder':'impala',\n",
        "    'Data_aug': 'cut_out',\n",
        "    'Mixreg': False,\n",
        "    'Policy': None,\n",
        "    'add_FCL': False\n",
        "}\n",
        "\n",
        "setup_run['ex6'] = {\n",
        "    'Encoder':'nature',\n",
        "    'Data_aug': 'regular',\n",
        "    'Mixreg': False,\n",
        "    'Policy': None,\n",
        "    'add_FCL': True\n",
        "}\n",
        "\n",
        "setup_run['ex7'] = {\n",
        "    'Encoder':'impala',\n",
        "    'Data_aug': 'regular',\n",
        "    'Mixreg': False,\n",
        "    'Policy': None,\n",
        "    'add_FCL': True\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTBV9xpKpEFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d643d3dc-6aff-4932-8477-ff9e78ade405"
      },
      "source": [
        "#  ============================       RUNNING LOOP      ==============================\n",
        "\n",
        "# run_setups =['Run1','Run2','Run3','Run4','Run5'] # Specify a list of setups to run\n",
        "#run_setups =['ex1','ex2','ex3','ex4','ex5', 'ex6', 'ex7'] # Specify a list of setups to run\n",
        "run_setups = ['ex6', 'ex7']\n",
        "\n",
        "for run in run_setups: # Run the training for all our setups\n",
        "  # Define environment\n",
        "  # check the utils.py file for info on arguments\n",
        "  env_name = 'starpilot'\n",
        "  env = make_env(num_envs, num_levels=num_levels, env_name='starpilot')\n",
        "  print('Observation space:', env.observation_space)\n",
        "  print('Action space:', env.action_space.n)\n",
        "  print(f'Run setup for {run}: playing {env_name}')\n",
        "  for setup_key in setup_run[run]:\n",
        "    print(f'{setup_key}: {setup_run[run][setup_key]}', end = '\\t')\n",
        "  print('')\n",
        "\n",
        "  # Read in the setup\n",
        "  do_mixreg = setup_run[run]['Mixreg']\n",
        "  data_aug = setup_run[run]['Data_aug']\n",
        "  encoder_use = setup_run[run]['Encoder']\n",
        "  additional_FCL = setup_run[run]['add_FCL']\n",
        "\n",
        "  # Define network\n",
        "  feature_dim = 512\n",
        "  lambda_mix = 0.95\n",
        "  num_actions = env.action_space.n\n",
        "  in_channels = env.observation_space.shape[0]\n",
        "\n",
        "  # Define the encoder\n",
        "  if encoder_use == 'impala':\n",
        "    print('Using Impala') \n",
        "    encoder = Encoder(in_channels, feature_dim) # added\n",
        "  else:\n",
        "    encoder = Encoder2(in_channels, feature_dim) # added\n",
        "\n",
        "  # Initialize the policy\n",
        "  if additional_FCL:\n",
        "    policy = Policy_add_FCL(encoder, feature_dim, num_actions)\n",
        "  else:\n",
        "    policy = Policy(encoder, feature_dim, num_actions)\n",
        "  policy.cuda()\n",
        "\n",
        "  # Define optimizer\n",
        "  # these are reasonable values but probably not optimal\n",
        "  optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
        "\n",
        "  # Define temporary storage\n",
        "  # we use this to collect transitions during each iteration\n",
        "  storage = Storage(\n",
        "      env.observation_space.shape,\n",
        "      num_steps,\n",
        "      num_envs\n",
        "  )\n",
        "\n",
        "  # Run training\n",
        "  obs = env.reset()\n",
        "  nenv = env.num_envs\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "  # Change the first observations to desired augmentation\n",
        "  if data_aug == 'grayscale':\n",
        "    obs = grayscale(obs,device)\n",
        "  elif data_aug == 'random_cutout':\n",
        "    # Initialize as a numpy array then convert to tensor\n",
        "    obs = np.zeros((nenv,) + env.observation_space.shape, dtype=env.observation_space.dtype.name)\n",
        "    obs[:] = env.reset()\n",
        "\n",
        "    # Do the cutout and transfer to tensor\n",
        "    obs = random_cutout(obs,12,24)\n",
        "    obs = torch.from_numpy(obs)\n",
        "  elif data_aug == 'color_jitter':\n",
        "    color_jitter(obs)\n",
        "\n",
        "  step = 0\n",
        "  # Initilize mean_reward for each setup that we store in the end\n",
        "  mean_rewards = []\n",
        "  mean_rewards_done = []\n",
        "  first_loop = True\n",
        "  start = time() # Lets measure how long each training task takes\n",
        "  while step < total_steps:\n",
        "    # Use policy to collect data for num_steps steps\n",
        "    policy.eval()\n",
        "    for _ in range(num_steps):\n",
        "      # Use policy\n",
        "      action, log_prob, value = policy.act(obs)\n",
        "      \n",
        "      # Take step in environment\n",
        "      next_obs = np.zeros((nenv,) + env.observation_space.shape, dtype=env.observation_space.dtype.name)\n",
        "\n",
        "      # numpy obs\n",
        "      next_obs[:], reward, done, info = env.step(action)\n",
        "\n",
        "      # Store data\n",
        "      storage.store(obs, action, reward, done, info, log_prob, value)\n",
        "      \n",
        "      # Update current observation\n",
        "      # obs = next_obs\n",
        "\n",
        "      # Make augmented transformation, probably possible to do this another way, like in a class to avoid the if statements\n",
        "      if data_aug == 'grayscale':\n",
        "        obs = torch.from_numpy(next_obs)\n",
        "        obs = grayscale(obs,device)\n",
        "      elif data_aug == 'random_cutout':\n",
        "        obs = random_cutout(next_obs,12,24)\n",
        "        obs = torch.from_numpy(obs)\n",
        "      elif data_aug == 'color_jitter':\n",
        "        obs = torch.from_numpy(next_obs)\n",
        "        color_jitter(obs)\n",
        "      else:\n",
        "        obs = torch.from_numpy(next_obs)\n",
        "\n",
        "    # Add the last observation to collected data\n",
        "    _, _, value = policy.act(obs)\n",
        "    storage.store_last(obs, value)\n",
        "\n",
        "    # Compute return and advantage\n",
        "    storage.compute_return_advantage()\n",
        "\n",
        "    # Optimize policy\n",
        "    policy.train()\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      # Iterate over batches of transitions\n",
        "      generator = storage.get_generator(batch_size)\n",
        "      for batch in generator:\n",
        "        b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "        if do_mixreg: \n",
        "          index_ij = torch.randint(0, batch_size-1, (batch_size,2))\n",
        "          b_obs = lambda_mix*b_obs[index_ij[:,0]] + (1-lambda_mix)*b_obs[index_ij[:,1]]\n",
        "          b_log_prob = lambda_mix*b_log_prob[index_ij[:,0]] + (1-lambda_mix)*b_log_prob[index_ij[:,1]]\n",
        "          b_value = lambda_mix*b_value[index_ij[:,0]] + (1-lambda_mix)*b_value[index_ij[:,1]]\n",
        "          b_returns = lambda_mix*b_returns[index_ij[:,0]] + (1-lambda_mix)*b_returns[index_ij[:,1]]\n",
        "          b_advantage = lambda_mix*b_advantage[index_ij[:,0]] + (1-lambda_mix)*b_advantage[index_ij[:,1]]\n",
        "          if (lambda_mix >= 0.5):\n",
        "            b_action = b_action[index_ij[:,0]]\n",
        "          else:\n",
        "            b_action = b_action[index_ij[:,1]]\n",
        "\n",
        "        # Get current policy outputs\n",
        "        new_dist, new_value = policy(b_obs)\n",
        "        new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "        # Clipped policy objective\n",
        "        ratio = torch.exp(new_log_prob - b_log_prob) # added\n",
        "        # ratio = b_log_prob/new_log_prob # added\n",
        "        clipped_ratio = ratio.clamp(min=1.0 - eps,max=1.0 + eps) # added\n",
        "        # pi_loss = torch.min(rt_theta*b_advantage,) # added\n",
        "        pi_loss = -torch.min(ratio * b_advantage,clipped_ratio * b_advantage).mean() # added\n",
        "\n",
        "        # Clipped value function objective\n",
        "        clipped_value = b_value + (new_value - b_value).clamp(min=-eps, max=eps) # added\n",
        "        # value_loss = (new_value - b_value)**2 # added\n",
        "        value_loss = 0.5 * torch.max((b_value - b_returns) ** 2, (clipped_value - b_returns) **2).mean() # added\n",
        "\n",
        "        # Entropy loss\n",
        "        entropy_loss = -new_dist.entropy().mean() # added\n",
        "\n",
        "        # Backpropagate losses\n",
        "        loss = pi_loss + value_coef*value_loss + entropy_coef*entropy_loss # added\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps) # added\n",
        "\n",
        "        # Update policy\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Update stats\n",
        "    #mean_rewards.append(storage.get_reward(normalized_reward=False))\n",
        "    mean_rewards.append(storage.get_reward())\n",
        "    done_reward = sum((sum(storage.reward)/(sum(storage.done)+1)))/num_envs\n",
        "\n",
        "    # TODO: If you never die implement an if statement that doesn't include the plus 1\n",
        "    mean_rewards_done.append(done_reward)\n",
        "    step += num_envs * num_steps\n",
        "    print(f'Step: {step}\\tMean reward: {storage.get_reward()}, \\tMean reward done: {done_reward}')\n",
        "    if first_loop:\n",
        "      end = time()\n",
        "      time_total = end-start\n",
        "      estimated_time = (8e6/(8192/time_total))/3600\n",
        "      print(f'Estimated time of completion: {estimated_time} hours')\n",
        "    first_loop = False\n",
        "  end = time()\n",
        "  time_total = end-start\n",
        "  # Save the newest version after every epoch\n",
        "  torch.save({\n",
        "              'Setup': setup_run[run], # Have \n",
        "              'policy_state_dict': policy.state_dict(), # This is the policy\n",
        "              'encoder_state_dict': encoder.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(), # The optimizer used\n",
        "              'Mean Reward': mean_rewards,\n",
        "              'Mean Reward Done': mean_rewards_done,\n",
        "              'Training time': time_total,\n",
        "              }, f'/content/gdrive/MyDrive/Deep Learning Project 2020/data/{run}.pt')\n",
        "  print(f'Completed training of {run}!')\n",
        "  torch.save(policy.state_dict(), f'{run}_policy.pt')\n",
        "\n",
        "# torch.save(policy.state_dict(), f'{run}_policy.pt')\n",
        "print('Completed all runs!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
            "Action space: 15\n",
            "Run setup for ex6: playing starpilot\n",
            "Encoder: nature\tData_aug: regular\tMixreg: False\tPolicy: None\tadd_FCL: True\t\n",
            "Step: 8192\tMean reward: 5.28125, \tMean reward done: 2.391862630844116\n",
            "Estimated time of completion: 3.250933109989597 hours\n",
            "Step: 16384\tMean reward: 6.125, \tMean reward done: 1.333879828453064\n",
            "Step: 24576\tMean reward: 5.875, \tMean reward done: 1.3695173263549805\n",
            "Step: 32768\tMean reward: 6.53125, \tMean reward done: 1.5071823596954346\n",
            "Step: 40960\tMean reward: 7.5625, \tMean reward done: 1.6400834321975708\n",
            "Step: 49152\tMean reward: 7.5625, \tMean reward done: 1.65478515625\n",
            "Step: 57344\tMean reward: 5.25, \tMean reward done: 1.1469238996505737\n",
            "Step: 65536\tMean reward: 8.78125, \tMean reward done: 2.026515007019043\n",
            "Step: 73728\tMean reward: 7.59375, \tMean reward done: 1.4697065353393555\n",
            "Step: 81920\tMean reward: 5.71875, \tMean reward done: 1.202273964881897\n",
            "Step: 90112\tMean reward: 7.78125, \tMean reward done: 1.643452525138855\n",
            "Step: 98304\tMean reward: 7.6875, \tMean reward done: 1.4998068809509277\n",
            "Step: 106496\tMean reward: 6.15625, \tMean reward done: 1.2684690952301025\n",
            "Step: 114688\tMean reward: 6.9375, \tMean reward done: 1.2424564361572266\n",
            "Step: 122880\tMean reward: 8.34375, \tMean reward done: 1.8663088083267212\n",
            "Step: 131072\tMean reward: 8.5, \tMean reward done: 1.7461249828338623\n",
            "Step: 139264\tMean reward: 8.40625, \tMean reward done: 1.8757954835891724\n",
            "Step: 147456\tMean reward: 8.84375, \tMean reward done: 1.804552674293518\n",
            "Step: 155648\tMean reward: 9.0, \tMean reward done: 2.0007622241973877\n",
            "Step: 163840\tMean reward: 8.9375, \tMean reward done: 1.929490089416504\n",
            "Step: 172032\tMean reward: 8.03125, \tMean reward done: 1.6537200212478638\n",
            "Step: 180224\tMean reward: 8.65625, \tMean reward done: 1.7091619968414307\n",
            "Step: 188416\tMean reward: 9.15625, \tMean reward done: 1.9179939031600952\n",
            "Step: 196608\tMean reward: 9.25, \tMean reward done: 1.7369767427444458\n",
            "Step: 204800\tMean reward: 9.0625, \tMean reward done: 1.772261142730713\n",
            "Step: 212992\tMean reward: 9.75, \tMean reward done: 1.8600730895996094\n",
            "Step: 221184\tMean reward: 8.9375, \tMean reward done: 1.8189822435379028\n",
            "Step: 229376\tMean reward: 8.875, \tMean reward done: 1.8321523666381836\n",
            "Step: 237568\tMean reward: 8.71875, \tMean reward done: 1.780192494392395\n",
            "Step: 245760\tMean reward: 8.21875, \tMean reward done: 1.6166226863861084\n",
            "Step: 253952\tMean reward: 9.25, \tMean reward done: 1.8304204940795898\n",
            "Step: 262144\tMean reward: 7.65625, \tMean reward done: 1.629866123199463\n",
            "Step: 270336\tMean reward: 8.28125, \tMean reward done: 1.6347631216049194\n",
            "Step: 278528\tMean reward: 8.5, \tMean reward done: 1.6973707675933838\n",
            "Step: 286720\tMean reward: 8.53125, \tMean reward done: 1.7037485837936401\n",
            "Step: 294912\tMean reward: 9.21875, \tMean reward done: 1.9472427368164062\n",
            "Step: 303104\tMean reward: 8.71875, \tMean reward done: 1.9427989721298218\n",
            "Step: 311296\tMean reward: 7.5625, \tMean reward done: 1.44879949092865\n",
            "Step: 319488\tMean reward: 8.375, \tMean reward done: 1.734139084815979\n",
            "Step: 327680\tMean reward: 7.21875, \tMean reward done: 1.4650218486785889\n",
            "Step: 335872\tMean reward: 8.53125, \tMean reward done: 1.5456418991088867\n",
            "Step: 344064\tMean reward: 9.34375, \tMean reward done: 1.8112149238586426\n",
            "Step: 352256\tMean reward: 9.5625, \tMean reward done: 1.8655415773391724\n",
            "Step: 360448\tMean reward: 8.78125, \tMean reward done: 1.8057501316070557\n",
            "Step: 368640\tMean reward: 8.96875, \tMean reward done: 1.7928816080093384\n",
            "Step: 376832\tMean reward: 9.6875, \tMean reward done: 1.9255263805389404\n",
            "Step: 385024\tMean reward: 8.9375, \tMean reward done: 1.9096094369888306\n",
            "Step: 393216\tMean reward: 9.75, \tMean reward done: 1.8890951871871948\n",
            "Step: 401408\tMean reward: 9.65625, \tMean reward done: 1.965377688407898\n",
            "Step: 409600\tMean reward: 8.53125, \tMean reward done: 1.8173844814300537\n",
            "Step: 417792\tMean reward: 8.3125, \tMean reward done: 1.6552214622497559\n",
            "Step: 425984\tMean reward: 9.28125, \tMean reward done: 1.8633579015731812\n",
            "Step: 434176\tMean reward: 7.96875, \tMean reward done: 1.463267207145691\n",
            "Step: 442368\tMean reward: 8.0625, \tMean reward done: 1.6075338125228882\n",
            "Step: 450560\tMean reward: 10.21875, \tMean reward done: 2.237816095352173\n",
            "Step: 458752\tMean reward: 10.0, \tMean reward done: 2.0070297718048096\n",
            "Step: 466944\tMean reward: 10.71875, \tMean reward done: 2.2281205654144287\n",
            "Step: 475136\tMean reward: 9.6875, \tMean reward done: 1.9667341709136963\n",
            "Step: 483328\tMean reward: 9.1875, \tMean reward done: 1.7682911157608032\n",
            "Step: 491520\tMean reward: 10.625, \tMean reward done: 1.8943970203399658\n",
            "Step: 499712\tMean reward: 10.0, \tMean reward done: 1.9234400987625122\n",
            "Step: 507904\tMean reward: 8.40625, \tMean reward done: 1.685590386390686\n",
            "Step: 516096\tMean reward: 9.28125, \tMean reward done: 1.8041139841079712\n",
            "Step: 524288\tMean reward: 9.9375, \tMean reward done: 2.05106782913208\n",
            "Step: 532480\tMean reward: 10.21875, \tMean reward done: 1.9713140726089478\n",
            "Step: 540672\tMean reward: 9.21875, \tMean reward done: 1.7350867986679077\n",
            "Step: 548864\tMean reward: 9.90625, \tMean reward done: 1.8868073225021362\n",
            "Step: 557056\tMean reward: 10.09375, \tMean reward done: 1.9214260578155518\n",
            "Step: 565248\tMean reward: 10.3125, \tMean reward done: 1.795553207397461\n",
            "Step: 573440\tMean reward: 10.375, \tMean reward done: 1.942568302154541\n",
            "Step: 581632\tMean reward: 9.0625, \tMean reward done: 1.6407859325408936\n",
            "Step: 589824\tMean reward: 9.65625, \tMean reward done: 1.9710549116134644\n",
            "Step: 598016\tMean reward: 10.96875, \tMean reward done: 2.197659492492676\n",
            "Step: 606208\tMean reward: 10.6875, \tMean reward done: 2.0679450035095215\n",
            "Step: 614400\tMean reward: 11.40625, \tMean reward done: 2.201510190963745\n",
            "Step: 622592\tMean reward: 10.625, \tMean reward done: 1.9386380910873413\n",
            "Step: 630784\tMean reward: 11.28125, \tMean reward done: 2.0648348331451416\n",
            "Step: 638976\tMean reward: 10.3125, \tMean reward done: 2.0840470790863037\n",
            "Step: 647168\tMean reward: 8.78125, \tMean reward done: 1.59047269821167\n",
            "Step: 655360\tMean reward: 10.75, \tMean reward done: 2.0974183082580566\n",
            "Step: 663552\tMean reward: 11.09375, \tMean reward done: 2.230809211730957\n",
            "Step: 671744\tMean reward: 9.9375, \tMean reward done: 1.9662631750106812\n",
            "Step: 679936\tMean reward: 9.0, \tMean reward done: 1.7176549434661865\n",
            "Step: 688128\tMean reward: 11.59375, \tMean reward done: 2.2885138988494873\n",
            "Step: 696320\tMean reward: 10.5625, \tMean reward done: 1.904189109802246\n",
            "Step: 704512\tMean reward: 11.125, \tMean reward done: 1.9741846323013306\n",
            "Step: 712704\tMean reward: 10.75, \tMean reward done: 2.0592780113220215\n",
            "Step: 720896\tMean reward: 9.28125, \tMean reward done: 1.8508193492889404\n",
            "Step: 729088\tMean reward: 11.0625, \tMean reward done: 2.363262414932251\n",
            "Step: 737280\tMean reward: 11.6875, \tMean reward done: 2.3847594261169434\n",
            "Step: 745472\tMean reward: 10.59375, \tMean reward done: 2.0541229248046875\n",
            "Step: 753664\tMean reward: 11.15625, \tMean reward done: 2.120957612991333\n",
            "Step: 761856\tMean reward: 10.53125, \tMean reward done: 1.9780023097991943\n",
            "Step: 770048\tMean reward: 12.34375, \tMean reward done: 2.243952751159668\n",
            "Step: 778240\tMean reward: 10.78125, \tMean reward done: 1.9605743885040283\n",
            "Step: 786432\tMean reward: 10.78125, \tMean reward done: 2.1084673404693604\n",
            "Step: 794624\tMean reward: 9.875, \tMean reward done: 1.9521993398666382\n",
            "Step: 802816\tMean reward: 10.6875, \tMean reward done: 2.0088963508605957\n",
            "Step: 811008\tMean reward: 11.125, \tMean reward done: 2.3624775409698486\n",
            "Step: 819200\tMean reward: 10.96875, \tMean reward done: 1.9086859226226807\n",
            "Step: 827392\tMean reward: 9.75, \tMean reward done: 1.6989234685897827\n",
            "Step: 835584\tMean reward: 11.28125, \tMean reward done: 2.138901710510254\n",
            "Step: 843776\tMean reward: 11.9375, \tMean reward done: 2.448452949523926\n",
            "Step: 851968\tMean reward: 9.0625, \tMean reward done: 1.7319742441177368\n",
            "Step: 860160\tMean reward: 10.9375, \tMean reward done: 2.089447498321533\n",
            "Step: 868352\tMean reward: 12.6875, \tMean reward done: 2.822343349456787\n",
            "Step: 876544\tMean reward: 8.71875, \tMean reward done: 1.6039873361587524\n",
            "Step: 884736\tMean reward: 10.375, \tMean reward done: 2.1564786434173584\n",
            "Step: 892928\tMean reward: 11.15625, \tMean reward done: 2.030073404312134\n",
            "Step: 901120\tMean reward: 9.96875, \tMean reward done: 2.024207592010498\n",
            "Step: 909312\tMean reward: 10.59375, \tMean reward done: 2.108283519744873\n",
            "Step: 917504\tMean reward: 11.96875, \tMean reward done: 2.4348127841949463\n",
            "Step: 925696\tMean reward: 10.90625, \tMean reward done: 2.075666666030884\n",
            "Step: 933888\tMean reward: 11.5625, \tMean reward done: 2.00142765045166\n",
            "Step: 942080\tMean reward: 11.1875, \tMean reward done: 1.9194434881210327\n",
            "Step: 950272\tMean reward: 10.90625, \tMean reward done: 1.984817624092102\n",
            "Step: 958464\tMean reward: 11.0, \tMean reward done: 2.1282005310058594\n",
            "Step: 966656\tMean reward: 10.40625, \tMean reward done: 2.0027260780334473\n",
            "Step: 974848\tMean reward: 8.96875, \tMean reward done: 1.751242995262146\n",
            "Step: 983040\tMean reward: 10.15625, \tMean reward done: 1.6951555013656616\n",
            "Step: 991232\tMean reward: 10.875, \tMean reward done: 2.084355354309082\n",
            "Step: 999424\tMean reward: 10.125, \tMean reward done: 1.7387264966964722\n",
            "Step: 1007616\tMean reward: 11.21875, \tMean reward done: 1.8894052505493164\n",
            "Step: 1015808\tMean reward: 11.40625, \tMean reward done: 1.9587514400482178\n",
            "Step: 1024000\tMean reward: 11.28125, \tMean reward done: 2.1693356037139893\n",
            "Step: 1032192\tMean reward: 10.5, \tMean reward done: 1.8639793395996094\n",
            "Step: 1040384\tMean reward: 11.3125, \tMean reward done: 2.0528485774993896\n",
            "Step: 1048576\tMean reward: 11.5, \tMean reward done: 2.1830978393554688\n",
            "Step: 1056768\tMean reward: 10.8125, \tMean reward done: 1.9646340608596802\n",
            "Step: 1064960\tMean reward: 10.8125, \tMean reward done: 1.9689005613327026\n",
            "Step: 1073152\tMean reward: 12.15625, \tMean reward done: 2.1737961769104004\n",
            "Step: 1081344\tMean reward: 11.71875, \tMean reward done: 2.0374608039855957\n",
            "Step: 1089536\tMean reward: 11.28125, \tMean reward done: 1.9478851556777954\n",
            "Step: 1097728\tMean reward: 11.1875, \tMean reward done: 1.9499621391296387\n",
            "Step: 1105920\tMean reward: 10.34375, \tMean reward done: 2.085826873779297\n",
            "Step: 1114112\tMean reward: 12.15625, \tMean reward done: 2.123673915863037\n",
            "Step: 1122304\tMean reward: 11.34375, \tMean reward done: 2.3406360149383545\n",
            "Step: 1130496\tMean reward: 10.875, \tMean reward done: 2.179802179336548\n",
            "Step: 1138688\tMean reward: 10.0625, \tMean reward done: 1.9048163890838623\n",
            "Step: 1146880\tMean reward: 12.5, \tMean reward done: 2.5539615154266357\n",
            "Step: 1155072\tMean reward: 13.25, \tMean reward done: 2.6545393466949463\n",
            "Step: 1163264\tMean reward: 11.25, \tMean reward done: 1.959803819656372\n",
            "Step: 1171456\tMean reward: 11.78125, \tMean reward done: 2.2396762371063232\n",
            "Step: 1179648\tMean reward: 11.46875, \tMean reward done: 2.1575160026550293\n",
            "Step: 1187840\tMean reward: 12.875, \tMean reward done: 2.3408713340759277\n",
            "Step: 1196032\tMean reward: 11.09375, \tMean reward done: 2.0578532218933105\n",
            "Step: 1204224\tMean reward: 13.0625, \tMean reward done: 2.494036912918091\n",
            "Step: 1212416\tMean reward: 11.375, \tMean reward done: 2.0540904998779297\n",
            "Step: 1220608\tMean reward: 9.78125, \tMean reward done: 1.9196593761444092\n",
            "Step: 1228800\tMean reward: 11.78125, \tMean reward done: 1.8669817447662354\n",
            "Step: 1236992\tMean reward: 11.6875, \tMean reward done: 2.476175308227539\n",
            "Step: 1245184\tMean reward: 10.9375, \tMean reward done: 1.8810466527938843\n",
            "Step: 1253376\tMean reward: 12.125, \tMean reward done: 2.1225531101226807\n",
            "Step: 1261568\tMean reward: 11.0, \tMean reward done: 2.191171646118164\n",
            "Step: 1269760\tMean reward: 12.4375, \tMean reward done: 2.2657227516174316\n",
            "Step: 1277952\tMean reward: 12.6875, \tMean reward done: 2.371084213256836\n",
            "Step: 1286144\tMean reward: 13.40625, \tMean reward done: 2.6612460613250732\n",
            "Step: 1294336\tMean reward: 11.75, \tMean reward done: 2.177403211593628\n",
            "Step: 1302528\tMean reward: 12.53125, \tMean reward done: 2.2893619537353516\n",
            "Step: 1310720\tMean reward: 10.90625, \tMean reward done: 1.7548781633377075\n",
            "Step: 1318912\tMean reward: 11.5, \tMean reward done: 2.0504651069641113\n",
            "Step: 1327104\tMean reward: 11.75, \tMean reward done: 2.2265052795410156\n",
            "Step: 1335296\tMean reward: 11.625, \tMean reward done: 2.024219274520874\n",
            "Step: 1343488\tMean reward: 12.09375, \tMean reward done: 2.1125590801239014\n",
            "Step: 1351680\tMean reward: 12.34375, \tMean reward done: 2.1486194133758545\n",
            "Step: 1359872\tMean reward: 11.28125, \tMean reward done: 1.9147701263427734\n",
            "Step: 1368064\tMean reward: 11.4375, \tMean reward done: 2.026050090789795\n",
            "Step: 1376256\tMean reward: 12.34375, \tMean reward done: 2.2817575931549072\n",
            "Step: 1384448\tMean reward: 12.09375, \tMean reward done: 2.093325614929199\n",
            "Step: 1392640\tMean reward: 11.96875, \tMean reward done: 2.0897207260131836\n",
            "Step: 1400832\tMean reward: 10.5, \tMean reward done: 1.793559193611145\n",
            "Step: 1409024\tMean reward: 11.28125, \tMean reward done: 1.964890480041504\n",
            "Step: 1417216\tMean reward: 11.125, \tMean reward done: 1.93887460231781\n",
            "Step: 1425408\tMean reward: 11.40625, \tMean reward done: 2.1168529987335205\n",
            "Step: 1433600\tMean reward: 10.28125, \tMean reward done: 1.6712015867233276\n",
            "Step: 1441792\tMean reward: 10.90625, \tMean reward done: 1.9815820455551147\n",
            "Step: 1449984\tMean reward: 10.59375, \tMean reward done: 1.9272575378417969\n",
            "Step: 1458176\tMean reward: 10.46875, \tMean reward done: 2.0324411392211914\n",
            "Step: 1466368\tMean reward: 11.1875, \tMean reward done: 1.8045045137405396\n",
            "Step: 1474560\tMean reward: 10.46875, \tMean reward done: 1.9059045314788818\n",
            "Step: 1482752\tMean reward: 10.4375, \tMean reward done: 1.846673846244812\n",
            "Step: 1490944\tMean reward: 10.875, \tMean reward done: 2.096200704574585\n",
            "Step: 1499136\tMean reward: 10.21875, \tMean reward done: 1.7372685670852661\n",
            "Step: 1507328\tMean reward: 11.78125, \tMean reward done: 2.155897855758667\n",
            "Step: 1515520\tMean reward: 12.21875, \tMean reward done: 2.259455680847168\n",
            "Step: 1523712\tMean reward: 11.40625, \tMean reward done: 2.2387468814849854\n",
            "Step: 1531904\tMean reward: 11.0, \tMean reward done: 1.8295676708221436\n",
            "Step: 1540096\tMean reward: 12.0, \tMean reward done: 2.118911027908325\n",
            "Step: 1548288\tMean reward: 11.65625, \tMean reward done: 2.0720503330230713\n",
            "Step: 1556480\tMean reward: 12.96875, \tMean reward done: 2.32334041595459\n",
            "Step: 1564672\tMean reward: 11.84375, \tMean reward done: 2.0041146278381348\n",
            "Step: 1572864\tMean reward: 11.90625, \tMean reward done: 2.113034963607788\n",
            "Step: 1581056\tMean reward: 12.09375, \tMean reward done: 2.2235589027404785\n",
            "Step: 1589248\tMean reward: 11.6875, \tMean reward done: 2.2130026817321777\n",
            "Step: 1597440\tMean reward: 10.15625, \tMean reward done: 1.789934754371643\n",
            "Step: 1605632\tMean reward: 11.03125, \tMean reward done: 1.909563422203064\n",
            "Step: 1613824\tMean reward: 10.375, \tMean reward done: 1.9878367185592651\n",
            "Step: 1622016\tMean reward: 11.0625, \tMean reward done: 2.45165753364563\n",
            "Step: 1630208\tMean reward: 11.8125, \tMean reward done: 2.419980049133301\n",
            "Step: 1638400\tMean reward: 11.6875, \tMean reward done: 2.285747766494751\n",
            "Step: 1646592\tMean reward: 10.28125, \tMean reward done: 1.7262991666793823\n",
            "Step: 1654784\tMean reward: 9.96875, \tMean reward done: 1.6628392934799194\n",
            "Step: 1662976\tMean reward: 11.46875, \tMean reward done: 2.050442695617676\n",
            "Step: 1671168\tMean reward: 10.03125, \tMean reward done: 1.9999803304672241\n",
            "Step: 1679360\tMean reward: 11.875, \tMean reward done: 2.4014337062835693\n",
            "Step: 1687552\tMean reward: 12.5, \tMean reward done: 2.536733388900757\n",
            "Step: 1695744\tMean reward: 11.84375, \tMean reward done: 2.3330960273742676\n",
            "Step: 1703936\tMean reward: 11.6875, \tMean reward done: 2.280642032623291\n",
            "Step: 1712128\tMean reward: 11.375, \tMean reward done: 2.1652848720550537\n",
            "Step: 1720320\tMean reward: 11.125, \tMean reward done: 2.0893898010253906\n",
            "Step: 1728512\tMean reward: 10.34375, \tMean reward done: 1.8217756748199463\n",
            "Step: 1736704\tMean reward: 11.65625, \tMean reward done: 2.2858691215515137\n",
            "Step: 1744896\tMean reward: 10.9375, \tMean reward done: 1.9510178565979004\n",
            "Step: 1753088\tMean reward: 11.9375, \tMean reward done: 2.4661662578582764\n",
            "Step: 1761280\tMean reward: 13.3125, \tMean reward done: 2.651641607284546\n",
            "Step: 1769472\tMean reward: 11.96875, \tMean reward done: 2.2239997386932373\n",
            "Step: 1777664\tMean reward: 11.9375, \tMean reward done: 2.1446287631988525\n",
            "Step: 1785856\tMean reward: 12.25, \tMean reward done: 2.441671848297119\n",
            "Step: 1794048\tMean reward: 11.96875, \tMean reward done: 2.1654160022735596\n",
            "Step: 1802240\tMean reward: 12.46875, \tMean reward done: 2.5461056232452393\n",
            "Step: 1810432\tMean reward: 11.75, \tMean reward done: 2.3800454139709473\n",
            "Step: 1818624\tMean reward: 11.0625, \tMean reward done: 2.0869874954223633\n",
            "Step: 1826816\tMean reward: 10.09375, \tMean reward done: 1.7531038522720337\n",
            "Step: 1835008\tMean reward: 11.46875, \tMean reward done: 2.0267105102539062\n",
            "Step: 1843200\tMean reward: 11.40625, \tMean reward done: 2.1025655269622803\n",
            "Step: 1851392\tMean reward: 12.625, \tMean reward done: 2.240337610244751\n",
            "Step: 1859584\tMean reward: 12.03125, \tMean reward done: 2.022747755050659\n",
            "Step: 1867776\tMean reward: 12.4375, \tMean reward done: 2.542983055114746\n",
            "Step: 1875968\tMean reward: 10.03125, \tMean reward done: 1.8332633972167969\n",
            "Step: 1884160\tMean reward: 12.34375, \tMean reward done: 2.0288548469543457\n",
            "Step: 1892352\tMean reward: 12.34375, \tMean reward done: 2.224961042404175\n",
            "Step: 1900544\tMean reward: 11.53125, \tMean reward done: 2.047272205352783\n",
            "Step: 1908736\tMean reward: 11.59375, \tMean reward done: 1.9783117771148682\n",
            "Step: 1916928\tMean reward: 10.4375, \tMean reward done: 1.7595757246017456\n",
            "Step: 1925120\tMean reward: 10.625, \tMean reward done: 2.130183219909668\n",
            "Step: 1933312\tMean reward: 11.84375, \tMean reward done: 2.0152251720428467\n",
            "Step: 1941504\tMean reward: 11.4375, \tMean reward done: 2.5360708236694336\n",
            "Step: 1949696\tMean reward: 11.3125, \tMean reward done: 2.080538749694824\n",
            "Step: 1957888\tMean reward: 10.8125, \tMean reward done: 1.8778005838394165\n",
            "Step: 1966080\tMean reward: 11.75, \tMean reward done: 2.2057862281799316\n",
            "Step: 1974272\tMean reward: 12.5625, \tMean reward done: 2.459272623062134\n",
            "Step: 1982464\tMean reward: 12.28125, \tMean reward done: 2.3039534091949463\n",
            "Step: 1990656\tMean reward: 12.28125, \tMean reward done: 2.2591562271118164\n",
            "Step: 1998848\tMean reward: 11.375, \tMean reward done: 1.9817475080490112\n",
            "Step: 2007040\tMean reward: 11.6875, \tMean reward done: 2.0604281425476074\n",
            "Completed training of ex6!\n",
            "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
            "Action space: 15\n",
            "Run setup for ex7: playing starpilot\n",
            "Encoder: impala\tData_aug: regular\tMixreg: False\tPolicy: None\tadd_FCL: True\t\n",
            "Using Impala\n",
            "Step: 8192\tMean reward: 5.15625, \tMean reward done: 2.370659351348877\n",
            "Estimated time of completion: 4.520887806494203 hours\n",
            "Step: 16384\tMean reward: 5.46875, \tMean reward done: 1.2880339622497559\n",
            "Step: 24576\tMean reward: 6.21875, \tMean reward done: 1.6200426816940308\n",
            "Step: 32768\tMean reward: 5.9375, \tMean reward done: 1.4732617139816284\n",
            "Step: 40960\tMean reward: 7.03125, \tMean reward done: 1.5698370933532715\n",
            "Step: 49152\tMean reward: 6.875, \tMean reward done: 1.6586601734161377\n",
            "Step: 57344\tMean reward: 5.78125, \tMean reward done: 1.3594412803649902\n",
            "Step: 65536\tMean reward: 7.75, \tMean reward done: 1.8609658479690552\n",
            "Step: 73728\tMean reward: 7.96875, \tMean reward done: 1.7290679216384888\n",
            "Step: 81920\tMean reward: 7.9375, \tMean reward done: 1.7448853254318237\n",
            "Step: 90112\tMean reward: 8.875, \tMean reward done: 1.9557602405548096\n",
            "Step: 98304\tMean reward: 7.46875, \tMean reward done: 1.51875901222229\n",
            "Step: 106496\tMean reward: 8.125, \tMean reward done: 1.6942752599716187\n",
            "Step: 114688\tMean reward: 9.09375, \tMean reward done: 1.823064923286438\n",
            "Step: 122880\tMean reward: 9.375, \tMean reward done: 1.9658374786376953\n",
            "Step: 131072\tMean reward: 9.1875, \tMean reward done: 1.8647174835205078\n",
            "Step: 139264\tMean reward: 10.25, \tMean reward done: 2.0710973739624023\n",
            "Step: 147456\tMean reward: 9.3125, \tMean reward done: 1.9160263538360596\n",
            "Step: 155648\tMean reward: 10.09375, \tMean reward done: 2.0845725536346436\n",
            "Step: 163840\tMean reward: 9.28125, \tMean reward done: 1.729152798652649\n",
            "Step: 172032\tMean reward: 11.34375, \tMean reward done: 2.510470390319824\n",
            "Step: 180224\tMean reward: 11.8125, \tMean reward done: 2.607475519180298\n",
            "Step: 188416\tMean reward: 11.25, \tMean reward done: 2.265502691268921\n",
            "Step: 196608\tMean reward: 10.1875, \tMean reward done: 1.9706854820251465\n",
            "Step: 204800\tMean reward: 11.75, \tMean reward done: 2.5561978816986084\n",
            "Step: 212992\tMean reward: 12.0, \tMean reward done: 2.34645938873291\n",
            "Step: 221184\tMean reward: 11.78125, \tMean reward done: 2.1949141025543213\n",
            "Step: 229376\tMean reward: 11.0625, \tMean reward done: 2.259528875350952\n",
            "Step: 237568\tMean reward: 12.84375, \tMean reward done: 2.5986249446868896\n",
            "Step: 245760\tMean reward: 9.96875, \tMean reward done: 1.9135453701019287\n",
            "Step: 253952\tMean reward: 10.78125, \tMean reward done: 2.059018611907959\n",
            "Step: 262144\tMean reward: 11.90625, \tMean reward done: 2.404330015182495\n",
            "Step: 270336\tMean reward: 11.3125, \tMean reward done: 2.118316888809204\n",
            "Step: 278528\tMean reward: 11.3125, \tMean reward done: 2.2933337688446045\n",
            "Step: 286720\tMean reward: 12.15625, \tMean reward done: 2.2137537002563477\n",
            "Step: 294912\tMean reward: 12.09375, \tMean reward done: 2.3844804763793945\n",
            "Step: 303104\tMean reward: 12.125, \tMean reward done: 2.2670562267303467\n",
            "Step: 311296\tMean reward: 11.71875, \tMean reward done: 2.2516326904296875\n",
            "Step: 319488\tMean reward: 12.4375, \tMean reward done: 2.383340358734131\n",
            "Step: 327680\tMean reward: 11.46875, \tMean reward done: 2.1027867794036865\n",
            "Step: 335872\tMean reward: 11.4375, \tMean reward done: 2.210674524307251\n",
            "Step: 344064\tMean reward: 11.75, \tMean reward done: 2.2029781341552734\n",
            "Step: 352256\tMean reward: 11.59375, \tMean reward done: 2.2120866775512695\n",
            "Step: 360448\tMean reward: 12.625, \tMean reward done: 2.625861167907715\n",
            "Step: 368640\tMean reward: 11.46875, \tMean reward done: 2.142214298248291\n",
            "Step: 376832\tMean reward: 11.78125, \tMean reward done: 2.117788076400757\n",
            "Step: 385024\tMean reward: 12.25, \tMean reward done: 2.112882137298584\n",
            "Step: 393216\tMean reward: 12.21875, \tMean reward done: 2.116969108581543\n",
            "Step: 401408\tMean reward: 11.75, \tMean reward done: 2.2396299839019775\n",
            "Step: 409600\tMean reward: 13.0, \tMean reward done: 2.3503501415252686\n",
            "Step: 417792\tMean reward: 13.0625, \tMean reward done: 2.4263992309570312\n",
            "Step: 425984\tMean reward: 12.0625, \tMean reward done: 2.0946199893951416\n",
            "Step: 434176\tMean reward: 12.0625, \tMean reward done: 2.132453203201294\n",
            "Step: 442368\tMean reward: 11.75, \tMean reward done: 1.9998364448547363\n",
            "Step: 450560\tMean reward: 13.03125, \tMean reward done: 2.211294174194336\n",
            "Step: 458752\tMean reward: 13.3125, \tMean reward done: 2.588184356689453\n",
            "Step: 466944\tMean reward: 10.875, \tMean reward done: 2.149101495742798\n",
            "Step: 475136\tMean reward: 12.1875, \tMean reward done: 2.1812305450439453\n",
            "Step: 483328\tMean reward: 12.3125, \tMean reward done: 2.21803617477417\n",
            "Step: 491520\tMean reward: 14.34375, \tMean reward done: 2.645775318145752\n",
            "Step: 499712\tMean reward: 13.09375, \tMean reward done: 2.438467025756836\n",
            "Step: 507904\tMean reward: 13.90625, \tMean reward done: 2.669807195663452\n",
            "Step: 516096\tMean reward: 12.71875, \tMean reward done: 2.216020345687866\n",
            "Step: 524288\tMean reward: 12.46875, \tMean reward done: 2.1713008880615234\n",
            "Step: 532480\tMean reward: 11.59375, \tMean reward done: 1.9065731763839722\n",
            "Step: 540672\tMean reward: 11.59375, \tMean reward done: 2.076359987258911\n",
            "Step: 548864\tMean reward: 14.09375, \tMean reward done: 2.6604013442993164\n",
            "Step: 557056\tMean reward: 12.03125, \tMean reward done: 2.0634138584136963\n",
            "Step: 565248\tMean reward: 12.21875, \tMean reward done: 2.1222565174102783\n",
            "Step: 573440\tMean reward: 14.1875, \tMean reward done: 2.7440531253814697\n",
            "Step: 581632\tMean reward: 13.46875, \tMean reward done: 2.3796985149383545\n",
            "Step: 589824\tMean reward: 13.1875, \tMean reward done: 2.2501113414764404\n",
            "Step: 598016\tMean reward: 12.6875, \tMean reward done: 2.2981653213500977\n",
            "Step: 606208\tMean reward: 12.53125, \tMean reward done: 2.159837245941162\n",
            "Step: 614400\tMean reward: 13.25, \tMean reward done: 2.333341121673584\n",
            "Step: 622592\tMean reward: 13.34375, \tMean reward done: 2.3109376430511475\n",
            "Step: 630784\tMean reward: 13.625, \tMean reward done: 2.13215970993042\n",
            "Step: 638976\tMean reward: 13.0625, \tMean reward done: 2.2788822650909424\n",
            "Step: 647168\tMean reward: 13.0625, \tMean reward done: 2.1307108402252197\n",
            "Step: 655360\tMean reward: 12.5, \tMean reward done: 2.289234161376953\n",
            "Step: 663552\tMean reward: 13.6875, \tMean reward done: 2.334120035171509\n",
            "Step: 671744\tMean reward: 13.5625, \tMean reward done: 2.263198137283325\n",
            "Step: 679936\tMean reward: 14.90625, \tMean reward done: 2.9673261642456055\n",
            "Step: 688128\tMean reward: 13.5, \tMean reward done: 2.494879961013794\n",
            "Step: 696320\tMean reward: 12.4375, \tMean reward done: 2.200901508331299\n",
            "Step: 704512\tMean reward: 12.8125, \tMean reward done: 2.1546716690063477\n",
            "Step: 712704\tMean reward: 14.125, \tMean reward done: 2.573920726776123\n",
            "Step: 720896\tMean reward: 13.375, \tMean reward done: 2.2449231147766113\n",
            "Step: 729088\tMean reward: 13.21875, \tMean reward done: 2.281909227371216\n",
            "Step: 737280\tMean reward: 13.25, \tMean reward done: 2.4442789554595947\n",
            "Step: 745472\tMean reward: 13.34375, \tMean reward done: 2.368452310562134\n",
            "Step: 753664\tMean reward: 12.75, \tMean reward done: 2.1368818283081055\n",
            "Step: 761856\tMean reward: 12.84375, \tMean reward done: 2.153353214263916\n",
            "Step: 770048\tMean reward: 12.40625, \tMean reward done: 2.2741408348083496\n",
            "Step: 778240\tMean reward: 13.9375, \tMean reward done: 2.5211856365203857\n",
            "Step: 786432\tMean reward: 13.4375, \tMean reward done: 2.2509877681732178\n",
            "Step: 794624\tMean reward: 14.1875, \tMean reward done: 2.186631917953491\n",
            "Step: 802816\tMean reward: 13.84375, \tMean reward done: 2.6601171493530273\n",
            "Step: 811008\tMean reward: 13.3125, \tMean reward done: 1.9911695718765259\n",
            "Step: 819200\tMean reward: 13.21875, \tMean reward done: 2.321105480194092\n",
            "Step: 827392\tMean reward: 13.21875, \tMean reward done: 2.228663444519043\n",
            "Step: 835584\tMean reward: 12.90625, \tMean reward done: 2.205092668533325\n",
            "Step: 843776\tMean reward: 14.34375, \tMean reward done: 2.556663990020752\n",
            "Step: 851968\tMean reward: 13.125, \tMean reward done: 2.3289496898651123\n",
            "Step: 860160\tMean reward: 14.75, \tMean reward done: 2.6729025840759277\n",
            "Step: 868352\tMean reward: 13.21875, \tMean reward done: 2.1806304454803467\n",
            "Step: 876544\tMean reward: 13.6875, \tMean reward done: 2.220813751220703\n",
            "Step: 884736\tMean reward: 13.90625, \tMean reward done: 2.302466869354248\n",
            "Step: 892928\tMean reward: 13.65625, \tMean reward done: 2.396049737930298\n",
            "Step: 901120\tMean reward: 13.65625, \tMean reward done: 2.321255683898926\n",
            "Step: 909312\tMean reward: 14.90625, \tMean reward done: 2.556309223175049\n",
            "Step: 917504\tMean reward: 14.71875, \tMean reward done: 2.777538299560547\n",
            "Step: 925696\tMean reward: 14.125, \tMean reward done: 2.323331594467163\n",
            "Step: 933888\tMean reward: 12.8125, \tMean reward done: 2.0298268795013428\n",
            "Step: 942080\tMean reward: 14.1875, \tMean reward done: 2.112705707550049\n",
            "Step: 950272\tMean reward: 11.625, \tMean reward done: 1.858323335647583\n",
            "Step: 958464\tMean reward: 13.0625, \tMean reward done: 2.3166234493255615\n",
            "Step: 966656\tMean reward: 14.375, \tMean reward done: 2.6140971183776855\n",
            "Step: 974848\tMean reward: 13.5625, \tMean reward done: 2.292759656906128\n",
            "Step: 983040\tMean reward: 13.03125, \tMean reward done: 2.129377841949463\n",
            "Step: 991232\tMean reward: 13.5625, \tMean reward done: 2.1823298931121826\n",
            "Step: 999424\tMean reward: 13.1875, \tMean reward done: 2.144655466079712\n",
            "Step: 1007616\tMean reward: 13.5, \tMean reward done: 2.133852243423462\n",
            "Step: 1015808\tMean reward: 12.96875, \tMean reward done: 2.1148808002471924\n",
            "Step: 1024000\tMean reward: 13.15625, \tMean reward done: 2.1707754135131836\n",
            "Step: 1032192\tMean reward: 15.6875, \tMean reward done: 2.5789527893066406\n",
            "Step: 1040384\tMean reward: 14.8125, \tMean reward done: 2.6815590858459473\n",
            "Step: 1048576\tMean reward: 14.90625, \tMean reward done: 3.1031203269958496\n",
            "Step: 1056768\tMean reward: 14.625, \tMean reward done: 2.729322671890259\n",
            "Step: 1064960\tMean reward: 13.75, \tMean reward done: 2.1570000648498535\n",
            "Step: 1073152\tMean reward: 13.90625, \tMean reward done: 2.258983612060547\n",
            "Step: 1081344\tMean reward: 14.03125, \tMean reward done: 2.366689920425415\n",
            "Step: 1089536\tMean reward: 14.21875, \tMean reward done: 2.2758774757385254\n",
            "Step: 1097728\tMean reward: 13.71875, \tMean reward done: 2.5604214668273926\n",
            "Step: 1105920\tMean reward: 13.75, \tMean reward done: 2.078829526901245\n",
            "Step: 1114112\tMean reward: 13.75, \tMean reward done: 2.575850248336792\n",
            "Step: 1122304\tMean reward: 14.84375, \tMean reward done: 2.492281913757324\n",
            "Step: 1130496\tMean reward: 14.0, \tMean reward done: 2.347747802734375\n",
            "Step: 1138688\tMean reward: 16.1875, \tMean reward done: 3.025214910507202\n",
            "Step: 1146880\tMean reward: 14.40625, \tMean reward done: 2.3433237075805664\n",
            "Step: 1155072\tMean reward: 17.4375, \tMean reward done: 3.704240560531616\n",
            "Step: 1163264\tMean reward: 13.5, \tMean reward done: 2.1481850147247314\n",
            "Step: 1171456\tMean reward: 14.15625, \tMean reward done: 2.300624370574951\n",
            "Step: 1179648\tMean reward: 15.28125, \tMean reward done: 2.4704744815826416\n",
            "Step: 1187840\tMean reward: 13.53125, \tMean reward done: 2.0727462768554688\n",
            "Step: 1196032\tMean reward: 12.90625, \tMean reward done: 2.06679630279541\n",
            "Step: 1204224\tMean reward: 13.28125, \tMean reward done: 2.0483736991882324\n",
            "Step: 1212416\tMean reward: 14.09375, \tMean reward done: 2.4376063346862793\n",
            "Step: 1220608\tMean reward: 13.21875, \tMean reward done: 1.9299715757369995\n",
            "Step: 1228800\tMean reward: 14.3125, \tMean reward done: 2.6527271270751953\n",
            "Step: 1236992\tMean reward: 13.6875, \tMean reward done: 2.210249423980713\n",
            "Step: 1245184\tMean reward: 13.15625, \tMean reward done: 2.049109935760498\n",
            "Step: 1253376\tMean reward: 14.78125, \tMean reward done: 2.2382383346557617\n",
            "Step: 1261568\tMean reward: 13.78125, \tMean reward done: 2.5094730854034424\n",
            "Step: 1269760\tMean reward: 15.5625, \tMean reward done: 2.6168181896209717\n",
            "Step: 1277952\tMean reward: 16.15625, \tMean reward done: 2.7368361949920654\n",
            "Step: 1286144\tMean reward: 14.625, \tMean reward done: 2.6014277935028076\n",
            "Step: 1294336\tMean reward: 13.71875, \tMean reward done: 2.0593271255493164\n",
            "Step: 1302528\tMean reward: 16.15625, \tMean reward done: 2.6576297283172607\n",
            "Step: 1310720\tMean reward: 13.96875, \tMean reward done: 2.3304178714752197\n",
            "Step: 1318912\tMean reward: 15.09375, \tMean reward done: 2.8618037700653076\n",
            "Step: 1327104\tMean reward: 14.09375, \tMean reward done: 2.233412742614746\n",
            "Step: 1335296\tMean reward: 13.09375, \tMean reward done: 2.1396472454071045\n",
            "Step: 1343488\tMean reward: 15.75, \tMean reward done: 2.653892755508423\n",
            "Step: 1351680\tMean reward: 13.96875, \tMean reward done: 2.0358545780181885\n",
            "Step: 1359872\tMean reward: 16.375, \tMean reward done: 2.779374361038208\n",
            "Step: 1368064\tMean reward: 14.25, \tMean reward done: 2.5271666049957275\n",
            "Step: 1376256\tMean reward: 16.46875, \tMean reward done: 3.1461405754089355\n",
            "Step: 1384448\tMean reward: 17.125, \tMean reward done: 2.749382495880127\n",
            "Step: 1392640\tMean reward: 15.65625, \tMean reward done: 2.9929115772247314\n",
            "Step: 1400832\tMean reward: 13.09375, \tMean reward done: 2.06330943107605\n",
            "Step: 1409024\tMean reward: 15.46875, \tMean reward done: 2.612055540084839\n",
            "Step: 1417216\tMean reward: 16.15625, \tMean reward done: 2.7638018131256104\n",
            "Step: 1425408\tMean reward: 17.0, \tMean reward done: 2.609543800354004\n",
            "Step: 1433600\tMean reward: 14.28125, \tMean reward done: 2.6511459350585938\n",
            "Step: 1441792\tMean reward: 15.21875, \tMean reward done: 2.3636605739593506\n",
            "Step: 1449984\tMean reward: 15.46875, \tMean reward done: 2.434709310531616\n",
            "Step: 1458176\tMean reward: 15.6875, \tMean reward done: 2.852034330368042\n",
            "Step: 1466368\tMean reward: 14.46875, \tMean reward done: 2.1949570178985596\n",
            "Step: 1474560\tMean reward: 15.96875, \tMean reward done: 2.655813217163086\n",
            "Step: 1482752\tMean reward: 16.28125, \tMean reward done: 2.886279582977295\n",
            "Step: 1490944\tMean reward: 15.6875, \tMean reward done: 2.5093462467193604\n",
            "Step: 1499136\tMean reward: 16.53125, \tMean reward done: 2.614431142807007\n",
            "Step: 1507328\tMean reward: 15.8125, \tMean reward done: 2.611483573913574\n",
            "Step: 1515520\tMean reward: 14.21875, \tMean reward done: 2.2764878273010254\n",
            "Step: 1523712\tMean reward: 14.375, \tMean reward done: 2.44569993019104\n",
            "Step: 1531904\tMean reward: 15.09375, \tMean reward done: 2.2530031204223633\n",
            "Step: 1540096\tMean reward: 13.875, \tMean reward done: 2.1774404048919678\n",
            "Step: 1548288\tMean reward: 15.625, \tMean reward done: 2.4131550788879395\n",
            "Step: 1556480\tMean reward: 14.125, \tMean reward done: 2.0281238555908203\n",
            "Step: 1564672\tMean reward: 15.5, \tMean reward done: 2.7022550106048584\n",
            "Step: 1572864\tMean reward: 15.9375, \tMean reward done: 2.335347890853882\n",
            "Step: 1581056\tMean reward: 14.96875, \tMean reward done: 2.4820966720581055\n",
            "Step: 1589248\tMean reward: 14.09375, \tMean reward done: 2.369720458984375\n",
            "Step: 1597440\tMean reward: 16.03125, \tMean reward done: 2.7945992946624756\n",
            "Step: 1605632\tMean reward: 16.125, \tMean reward done: 2.433030366897583\n",
            "Step: 1613824\tMean reward: 14.21875, \tMean reward done: 2.1426005363464355\n",
            "Step: 1622016\tMean reward: 15.34375, \tMean reward done: 2.6193785667419434\n",
            "Step: 1630208\tMean reward: 16.125, \tMean reward done: 2.6545777320861816\n",
            "Step: 1638400\tMean reward: 14.90625, \tMean reward done: 2.687387466430664\n",
            "Step: 1646592\tMean reward: 16.09375, \tMean reward done: 2.5904152393341064\n",
            "Step: 1654784\tMean reward: 14.9375, \tMean reward done: 2.5865159034729004\n",
            "Step: 1662976\tMean reward: 16.46875, \tMean reward done: 2.5959622859954834\n",
            "Step: 1671168\tMean reward: 13.78125, \tMean reward done: 2.17468523979187\n",
            "Step: 1679360\tMean reward: 17.46875, \tMean reward done: 2.8812918663024902\n",
            "Step: 1687552\tMean reward: 18.28125, \tMean reward done: 3.558072566986084\n",
            "Step: 1695744\tMean reward: 17.6875, \tMean reward done: 2.8791465759277344\n",
            "Step: 1703936\tMean reward: 15.34375, \tMean reward done: 2.663353681564331\n",
            "Step: 1712128\tMean reward: 15.3125, \tMean reward done: 2.719172477722168\n",
            "Step: 1720320\tMean reward: 16.875, \tMean reward done: 2.516937255859375\n",
            "Step: 1728512\tMean reward: 16.28125, \tMean reward done: 3.007293701171875\n",
            "Step: 1736704\tMean reward: 15.59375, \tMean reward done: 2.394547939300537\n",
            "Step: 1744896\tMean reward: 15.6875, \tMean reward done: 2.467283248901367\n",
            "Step: 1753088\tMean reward: 16.84375, \tMean reward done: 2.627141237258911\n",
            "Step: 1761280\tMean reward: 17.28125, \tMean reward done: 2.715656280517578\n",
            "Step: 1769472\tMean reward: 16.5, \tMean reward done: 2.797394037246704\n",
            "Step: 1777664\tMean reward: 17.0, \tMean reward done: 2.717118263244629\n",
            "Step: 1785856\tMean reward: 16.53125, \tMean reward done: 2.518946647644043\n",
            "Step: 1794048\tMean reward: 16.875, \tMean reward done: 2.897683620452881\n",
            "Step: 1802240\tMean reward: 15.46875, \tMean reward done: 2.3852593898773193\n",
            "Step: 1810432\tMean reward: 16.5625, \tMean reward done: 2.4822447299957275\n",
            "Step: 1818624\tMean reward: 17.09375, \tMean reward done: 2.8365139961242676\n",
            "Step: 1826816\tMean reward: 16.46875, \tMean reward done: 2.7185897827148438\n",
            "Step: 1835008\tMean reward: 16.8125, \tMean reward done: 2.6784181594848633\n",
            "Step: 1843200\tMean reward: 17.34375, \tMean reward done: 3.042060136795044\n",
            "Step: 1851392\tMean reward: 15.28125, \tMean reward done: 2.3702282905578613\n",
            "Step: 1859584\tMean reward: 16.84375, \tMean reward done: 2.530142068862915\n",
            "Step: 1867776\tMean reward: 16.28125, \tMean reward done: 2.5773444175720215\n",
            "Step: 1875968\tMean reward: 16.0625, \tMean reward done: 2.4948432445526123\n",
            "Step: 1884160\tMean reward: 16.1875, \tMean reward done: 2.819103479385376\n",
            "Step: 1892352\tMean reward: 14.8125, \tMean reward done: 2.545384407043457\n",
            "Step: 1900544\tMean reward: 15.34375, \tMean reward done: 2.2760162353515625\n",
            "Step: 1908736\tMean reward: 16.875, \tMean reward done: 2.532552719116211\n",
            "Step: 1916928\tMean reward: 16.0, \tMean reward done: 2.729011297225952\n",
            "Step: 1925120\tMean reward: 18.59375, \tMean reward done: 3.111868143081665\n",
            "Step: 1933312\tMean reward: 17.75, \tMean reward done: 2.9836041927337646\n",
            "Step: 1941504\tMean reward: 15.34375, \tMean reward done: 2.4392738342285156\n",
            "Step: 1949696\tMean reward: 16.3125, \tMean reward done: 2.504093885421753\n",
            "Step: 1957888\tMean reward: 16.21875, \tMean reward done: 2.595752000808716\n",
            "Step: 1966080\tMean reward: 15.375, \tMean reward done: 2.2982702255249023\n",
            "Step: 1974272\tMean reward: 14.65625, \tMean reward done: 2.280339241027832\n",
            "Step: 1982464\tMean reward: 17.21875, \tMean reward done: 2.8171026706695557\n",
            "Step: 1990656\tMean reward: 16.78125, \tMean reward done: 2.7020068168640137\n",
            "Step: 1998848\tMean reward: 15.4375, \tMean reward done: 2.3868634700775146\n",
            "Step: 2007040\tMean reward: 16.25, \tMean reward done: 2.4993529319763184\n",
            "Completed training of ex7!\n",
            "Completed all runs!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sto_cQWyaLkm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}